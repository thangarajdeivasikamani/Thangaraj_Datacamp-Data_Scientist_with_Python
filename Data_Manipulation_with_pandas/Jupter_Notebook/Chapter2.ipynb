{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>type</th>\n",
       "      <th>department</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>fuel_price_usd_per_l</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2/5/2010</td>\n",
       "      <td>4488.2</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>8.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2/12/2010</td>\n",
       "      <td>4105.8</td>\n",
       "      <td>True</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>8.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2/19/2010</td>\n",
       "      <td>4244.4</td>\n",
       "      <td>False</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>8.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2/26/2010</td>\n",
       "      <td>4919.1</td>\n",
       "      <td>False</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>8.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>3/5/2010</td>\n",
       "      <td>4912.5</td>\n",
       "      <td>False</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>8.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store type  department       date  weekly_sales  is_holiday  temperature_c  \\\n",
       "0      1    A           1   2/5/2010        4488.2       False          42.31   \n",
       "1      1    A           1  2/12/2010        4105.8        True          38.51   \n",
       "2      1    A           1  2/19/2010        4244.4       False          39.93   \n",
       "3      1    A           1  2/26/2010        4919.1       False          46.63   \n",
       "4      1    A           1   3/5/2010        4912.5       False          46.50   \n",
       "\n",
       "   fuel_price_usd_per_l  unemployment  MarkDown1  MarkDown2  MarkDown3  \\\n",
       "0                 2.572         8.106        NaN        NaN        NaN   \n",
       "1                 2.548         8.106        NaN        NaN        NaN   \n",
       "2                 2.514         8.106        NaN        NaN        NaN   \n",
       "3                 2.561         8.106        NaN        NaN        NaN   \n",
       "4                 2.625         8.106        NaN        NaN        NaN   \n",
       "\n",
       "   MarkDown4  MarkDown5         CPI  \n",
       "0        NaN        NaN  211.096358  \n",
       "1        NaN        NaN  211.242170  \n",
       "2        NaN        NaN  211.289143  \n",
       "3        NaN        NaN  211.319643  \n",
       "4        NaN        NaN  211.350143  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "walmart_sales_file= r'C:\\Machinelearning\\datacamp\\Thangaraj_Datacamp-Data_Scientist_with_Python\\Data_Manipulation_with_pandas\\Datasets\\walmart.csv'\n",
    "sales = pd.read_csv(walmart_sales_file)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max of given data set is 10\n",
      "min of given data set is 1\n",
      "Mean/Average of given data set is 5.5\n",
      "Mode of given data set is 4\n",
      "Median of given data set is 5.5\n",
      "std of given data set is 3.0276503540974917\n",
      "Variance of given data set is 9.166666666666666\n"
     ]
    }
   ],
   "source": [
    "#https://www.tutorialspoint.com/numpy/numpy_statistical_functions.htm\n",
    "#https://docs.python.org/3.4/library/statistics.html#statistics.mode\n",
    "import statistics\n",
    "list_numbers = [1,2,3,4,5,6,7,8,9,10]\n",
    "set1 =[1, 2, 3, 3, 4, 4, 4, 5, 5, 6] \n",
    "print(\"max of given data set is % s\" % max(list_numbers))\n",
    "print(\"min of given data set is % s\" % min(list_numbers))\n",
    "print(\"Mean/Average of given data set is % s\" % statistics.mean(list_numbers))\n",
    "print(\"Mode of given data set is % s\" % (statistics.mode(set1))) \n",
    "print(\"Median of given data set is % s\" % (statistics.median(list_numbers))) \n",
    "print(\"std of given data set is % s\" % (statistics.stdev(list_numbers))) \n",
    "print(\"Variance of given data set is % s\" % (statistics.variance(list_numbers)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Mean and median\n",
    "\n",
    "Summary statistics are exactly what they sound like - they summarize many numbers in one statistic. For example, mean, median, minimum, maximum, and standard deviation are summary statistics. Calculating summary statistics allows you to get a better sense of your data, even if there's a lot of it.\n",
    "\n",
    "`sales` is available and `pandas` is loaded as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Explore your new DataFrame first by printing the first few rows of the `sales` DataFrame.\n",
    "* Print information about the columns in `sales`.\n",
    "* Print the mean of the `weekly_sales` column.\n",
    "* Print the median of the `weekly_sales` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store type  department       date  weekly_sales  is_holiday  temperature_c  \\\n",
      "0      1    A           1   2/5/2010        4488.2       False          42.31   \n",
      "1      1    A           1  2/12/2010        4105.8        True          38.51   \n",
      "2      1    A           1  2/19/2010        4244.4       False          39.93   \n",
      "3      1    A           1  2/26/2010        4919.1       False          46.63   \n",
      "4      1    A           1   3/5/2010        4912.5       False          46.50   \n",
      "\n",
      "   fuel_price_usd_per_l  unemployment  MarkDown1  MarkDown2  MarkDown3  \\\n",
      "0                 2.572         8.106        NaN        NaN        NaN   \n",
      "1                 2.548         8.106        NaN        NaN        NaN   \n",
      "2                 2.514         8.106        NaN        NaN        NaN   \n",
      "3                 2.561         8.106        NaN        NaN        NaN   \n",
      "4                 2.625         8.106        NaN        NaN        NaN   \n",
      "\n",
      "   MarkDown4  MarkDown5         CPI  \n",
      "0        NaN        NaN  211.096358  \n",
      "1        NaN        NaN  211.242170  \n",
      "2        NaN        NaN  211.289143  \n",
      "3        NaN        NaN  211.319643  \n",
      "4        NaN        NaN  211.350143  \n",
      "---------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8190 entries, 0 to 8189\n",
      "Data columns (total 15 columns):\n",
      "store                   8190 non-null int64\n",
      "type                    8190 non-null object\n",
      "department              8190 non-null int64\n",
      "date                    8190 non-null object\n",
      "weekly_sales            8190 non-null float64\n",
      "is_holiday              8190 non-null bool\n",
      "temperature_c           8190 non-null float64\n",
      "fuel_price_usd_per_l    8190 non-null float64\n",
      "unemployment            7605 non-null float64\n",
      "MarkDown1               4032 non-null float64\n",
      "MarkDown2               2921 non-null float64\n",
      "MarkDown3               3613 non-null float64\n",
      "MarkDown4               3464 non-null float64\n",
      "MarkDown5               4050 non-null float64\n",
      "CPI                     7605 non-null float64\n",
      "dtypes: bool(1), float64(10), int64(2), object(2)\n",
      "memory usage: 903.9+ KB\n",
      "None\n",
      "---------------------\n",
      "6276.218962148973\n",
      "---------------------\n",
      "6421.65\n",
      "---------------------\n",
      "1872.7241644421704\n",
      "---------------------\n",
      "0    6712.4\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "print('---------------------')\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "print('---------------------')\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "print('---------------------')\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())\n",
    "print('---------------------')\n",
    "# Print the std of weekly_sales\n",
    "print(sales[\"weekly_sales\"].std())\n",
    "print('---------------------')\n",
    "# Print the mode of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing dates\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Summarizing dates\n",
    "\n",
    "Summary statistics can also be calculated on date columns which have values with the data type `datetime64`. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example minimum and maximum, which allow you to see what time range your data covers.\n",
    "\n",
    "`sales` is available and `pandas` is loaded as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Print the maximum of the date column.\n",
    "* Print the minimum of the date column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9/2011\n",
      "1/11/2013\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient summaries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Efficient summaries\n",
    "\n",
    "While pandas and NumPy have tons of functions, sometimes you may need a different function to summarize your data.\n",
    "\n",
    "The `.agg()` method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super efficient.\n",
    "\n",
    "In the custom function for this exercise, \"IQR\" is short for inter-quartile range, which is the 75th percentile minus the 25th percentile. It's an alternative to standard deviation that is helpful if your data contains outliers.\n",
    "\n",
    "`sales` is available and `pandas` is loaded as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions 1/3**\n",
    "\n",
    "Use the custom `iqr` function defined for you along with `.agg()` to print the IQR of the `temperature_c` column of `sales`.\n",
    "\n",
    "**Instructions 2/3**\n",
    "\n",
    "Update the column selection to use the custom `iqr` function with `.agg()` to print the IQR of `temperature_c`, `fuel_price_usd_per_l`, and `unemployment`, in that order.\n",
    "\n",
    "**Instructions 3/3**\n",
    "\n",
    "Update the aggregation functions called by `.agg(`): include `iqr` and `np.median` in that order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.9775\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_c           27.9775\n",
      "fuel_price_usd_per_l     0.7020\n",
      "unemployment             1.9330\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\",'fuel_price_usd_per_l','unemployment']].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        temperature_c  fuel_price_usd_per_l  unemployment\n",
      "iqr           27.9775                 0.702         1.933\n",
      "median        60.7100                 3.513         7.806\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative statistics\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Cumulative statistics\n",
    "\n",
    "Cumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you'll calculate the cumulative sum and cumulative max of a department's weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far.\n",
    "\n",
    "A DataFrame called `sales_1_1` has been created for you, which contains the sales data for department 1 of store 1. `pandas` is loaded as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Sort the rows of `sales_1_1` by the `date` column in ascending order.\n",
    "* Get the cumulative sum of `weekly_sales` and add it as a new column of `sales_1_1` called `cum_weekly_sales`.\n",
    "* Get the cumulative maximum of `weekly_sales`, and add it as a column called `cum_max_sales`.\n",
    "* Print the `date`, `weekly_sales`, `cum_weekly_sales`, and `cum_max_sales` columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_1_1 = sales[sales['department']==2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cumulative statistics\n",
    "\n",
    "Cumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you'll calculate the cumulative sum and cumulative max of a department's weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far.\n",
    "\n",
    "A DataFrame called sales_1_1 has been created for you, which contains the sales data for department 1 of store 1. pandas is loaded as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Sort the rows of sales_1_1 by the date column in ascending order.\n",
    "    Get the cumulative sum of weekly_sales and add it as a new column of sales_1_1 called cum_weekly_sales.\n",
    "    Get the cumulative maximum of weekly_sales, and add it as a column called cum_max_sales.\n",
    "    Print the date, weekly_sales, cum_weekly_sales, and cum_max_sales columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 15)\n",
      "           date  weekly_sales  cum_weekly_sales  cum_weekly_sales\n",
      "1245  1/11/2013        -365.3            -365.3            -365.3\n",
      "1609  1/11/2013        4571.3            4206.0            4206.0\n",
      "1791  1/11/2013        5292.2            9498.2            9498.2\n",
      "1973  1/11/2013        5357.3           14855.5           14855.5\n",
      "1427  1/11/2013        4203.3           19058.8           19058.8\n",
      "1063  1/11/2013        5150.3           24209.1           24209.1\n",
      "1557  1/13/2012        4570.1           28779.2           28779.2\n",
      "1739  1/13/2012        5447.7           34226.9           34226.9\n",
      "1375  1/13/2012        3972.1           38199.0           38199.0\n",
      "1921  1/13/2012        5636.1           43835.1           43835.1\n",
      "1011  1/13/2012        5159.1           48994.2           48994.2\n",
      "1193  1/13/2012        1053.7           50047.9           50047.9\n",
      "1505  1/14/2011        3941.3           53989.2           53989.2\n",
      "1687  1/14/2011        5329.2           59318.4           59318.4\n",
      "1323  1/14/2011        3460.3           62778.7           62778.7\n",
      "1869  1/14/2011        4832.3           67611.0           67611.0\n",
      "1141  1/14/2011        1423.1           69034.1           69034.1\n",
      "1610  1/18/2013        3957.7           72991.8           72991.8\n",
      "1246  1/18/2013        -440.1           72991.8           72991.8\n",
      "1428  1/18/2013        3506.7           76058.4           76058.4\n",
      "1064  1/18/2013        4817.7           80876.1           80876.1\n",
      "1792  1/18/2013        4421.5           85297.6           85297.6\n",
      "1974  1/18/2013        5290.7           90588.3           90588.3\n",
      "1194  1/20/2012        3046.5           93634.8           93634.8\n",
      "1376  1/20/2012        5007.8           98642.6           98642.6\n",
      "1012  1/20/2012        5863.8          104506.4          104506.4\n",
      "1558  1/20/2012        5482.8          109989.2          109989.2\n",
      "1740  1/20/2012        5817.0          115806.2          115806.2\n",
      "1922  1/20/2012        5969.8          121776.0          121776.0\n",
      "1870  1/21/2011        5452.6          127228.6          127228.6\n",
      "...         ...           ...               ...               ...\n",
      "1489  9/24/2010        7977.4         6306652.6         6306652.6\n",
      "1307  9/24/2010        7483.4         6314136.0         6314136.0\n",
      "1853  9/24/2010        7998.4         6322134.4         6322134.4\n",
      "1230  9/28/2012        5442.9         6327577.3         6327577.3\n",
      "1594  9/28/2012        8046.6         6335623.9         6335623.9\n",
      "1412  9/28/2012        7476.6         6343100.5         6343100.5\n",
      "1776  9/28/2012        8648.6         6351749.1         6351749.1\n",
      "1048  9/28/2012        8148.6         6359897.7         6359897.7\n",
      "1958  9/28/2012        8133.6         6368031.3         6368031.3\n",
      "1304   9/3/2010        7871.7         6375903.0         6375903.0\n",
      "1850   9/3/2010        8751.7         6384654.7         6384654.7\n",
      "1486   9/3/2010        8504.7         6393159.4         6393159.4\n",
      "1122   9/3/2010        5261.3         6398420.7         6398420.7\n",
      "1668   9/3/2010        8688.7         6407109.4         6407109.4\n",
      "1360  9/30/2011        7555.5         6414664.9         6414664.9\n",
      "1906  9/30/2011        8775.5         6423440.4         6423440.4\n",
      "1178  9/30/2011        5087.8         6428528.2         6428528.2\n",
      "1542  9/30/2011        8180.5         6436708.7         6436708.7\n",
      "1724  9/30/2011        8614.7         6445323.4         6445323.4\n",
      "1773   9/7/2012        8719.4         6454042.8         6454042.8\n",
      "1227   9/7/2012        6143.6         6460186.4         6460186.4\n",
      "1045   9/7/2012        9006.0         6469192.4         6469192.4\n",
      "1591   9/7/2012        9166.0         6478358.4         6478358.4\n",
      "1409   9/7/2012        8460.0         6486818.4         6486818.4\n",
      "1955   9/7/2012        8890.0         6495708.4         6495708.4\n",
      "1175   9/9/2011        4917.6         6500626.0         6500626.0\n",
      "1539   9/9/2011        7919.6         6508545.6         6508545.6\n",
      "1903   9/9/2011        8845.6         6517391.2         6517391.2\n",
      "1721   9/9/2011        9283.1         6526674.3         6526674.3\n",
      "1357   9/9/2011        7255.6         6533929.9         6533929.9\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values('date')\n",
    "print(sales_1_1.shape)\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum() # Means will add cumulatively e.g 1+2+3= 6\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1['cum_weekly_sales'] = sales_1_1[['cum_weekly_sales']].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_weekly_sales\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping duplicates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Dropping duplicates\n",
    "\n",
    "Removing duplicates is an essential skill to get accurate counts, because often you don't want to count the same thing multiple times. In this exercise, you'll create some new DataFrames using unique values from `sales`.\n",
    "\n",
    "`sales` is available and `pandas` is imported as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Remove rows of `sales` with duplicate pairs of `store` and `type` and save as `store_types` and print the head.\n",
    "* Remove rows of `sales` with duplicate pairs of `store` and `department` and save as `store_depts` and print the head.\n",
    "* Subset the rows that are holiday weeks, and drop the duplicate `dates`, saving as `holiday_dates`.\n",
    "Select the date column of `holiday_dates`, and print.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     store type  department      date  weekly_sales  is_holiday  \\\n",
      "0        1    A           1  2/5/2010        4488.2       False   \n",
      "182      2    A           1  2/5/2010        4276.2       False   \n",
      "364      3    A           1  2/5/2010        4828.2       False   \n",
      "546      4    A           1  2/5/2010        4635.8       False   \n",
      "728      5    A           1  2/5/2010        4227.2       False   \n",
      "\n",
      "     temperature_c  fuel_price_usd_per_l  unemployment  MarkDown1  MarkDown2  \\\n",
      "0            42.31                 2.572         8.106        NaN        NaN   \n",
      "182          40.19                 2.572         8.324        NaN        NaN   \n",
      "364          45.71                 2.572         7.368        NaN        NaN   \n",
      "546          43.76                 2.598         8.623        NaN        NaN   \n",
      "728          39.70                 2.572         6.566        NaN        NaN   \n",
      "\n",
      "     MarkDown3  MarkDown4  MarkDown5         CPI  \n",
      "0          NaN        NaN        NaN  211.096358  \n",
      "182        NaN        NaN        NaN  210.752605  \n",
      "364        NaN        NaN        NaN  214.424881  \n",
      "546        NaN        NaN        NaN  126.442065  \n",
      "728        NaN        NaN        NaN  211.653972  \n",
      "----------------------------\n",
      "     store type  department      date  weekly_sales  is_holiday  \\\n",
      "0        1    A           1  2/5/2010        4488.2       False   \n",
      "182      2    A           1  2/5/2010        4276.2       False   \n",
      "364      3    A           1  2/5/2010        4828.2       False   \n",
      "546      4    A           1  2/5/2010        4635.8       False   \n",
      "728      5    A           1  2/5/2010        4227.2       False   \n",
      "\n",
      "     temperature_c  fuel_price_usd_per_l  unemployment  MarkDown1  MarkDown2  \\\n",
      "0            42.31                 2.572         8.106        NaN        NaN   \n",
      "182          40.19                 2.572         8.324        NaN        NaN   \n",
      "364          45.71                 2.572         7.368        NaN        NaN   \n",
      "546          43.76                 2.598         8.623        NaN        NaN   \n",
      "728          39.70                 2.572         6.566        NaN        NaN   \n",
      "\n",
      "     MarkDown3  MarkDown4  MarkDown5         CPI  \n",
      "0          NaN        NaN        NaN  211.096358  \n",
      "182        NaN        NaN        NaN  210.752605  \n",
      "364        NaN        NaN        NaN  214.424881  \n",
      "546        NaN        NaN        NaN  126.442065  \n",
      "728        NaN        NaN        NaN  211.653972  \n",
      "----------------------------\n",
      "     store type  department        date  weekly_sales  is_holiday  \\\n",
      "1        1    A           1   2/12/2010        4105.8        True   \n",
      "31       1    A           1   9/10/2010        8125.5        True   \n",
      "42       1    A           1  11/26/2010        6725.5        True   \n",
      "47       1    A           1  12/31/2010        5137.3        True   \n",
      "53       1    A           1   2/11/2011        3941.2        True   \n",
      "83       1    A           1    9/9/2011        7954.6        True   \n",
      "94       1    A           1  11/25/2011        6337.6        True   \n",
      "99       1    A           1  12/30/2011        4767.9        True   \n",
      "105      1    A           1   2/10/2012        5142.9        True   \n",
      "135      1    A           1    9/7/2012        8769.0        True   \n",
      "146      1    A           1  11/23/2012        5944.1        True   \n",
      "151      1    A           1  12/28/2012        4789.8        True   \n",
      "157      1    A           1    2/8/2013        6008.7        True   \n",
      "\n",
      "     temperature_c  fuel_price_usd_per_l  unemployment  MarkDown1  MarkDown2  \\\n",
      "1            38.51                 2.548         8.106        NaN        NaN   \n",
      "31           78.69                 2.565         7.787        NaN        NaN   \n",
      "42           64.52                 2.735         7.838        NaN        NaN   \n",
      "47           48.43                 2.943         7.838        NaN        NaN   \n",
      "53           36.39                 3.022         7.742        NaN        NaN   \n",
      "83           76.00                 3.546         7.962        NaN        NaN   \n",
      "94           60.14                 3.236         7.866     410.31      98.00   \n",
      "99           44.55                 3.129         7.866    5762.10   46011.38   \n",
      "105          48.02                 3.409         7.348   13925.06    6927.23   \n",
      "135          83.96                 3.730         6.908    5204.68      35.74   \n",
      "146          56.23                 3.211         6.573     883.59       4.17   \n",
      "151          44.79                 3.108         6.573   12659.55   37101.13   \n",
      "157          56.67                 3.417         6.525   32355.16     729.80   \n",
      "\n",
      "     MarkDown3  MarkDown4  MarkDown5         CPI  \n",
      "1          NaN        NaN        NaN  211.242170  \n",
      "31         NaN        NaN        NaN  211.495190  \n",
      "42         NaN        NaN        NaN  211.748433  \n",
      "47         NaN        NaN        NaN  211.404932  \n",
      "53         NaN        NaN        NaN  212.936705  \n",
      "83         NaN        NaN        NaN  215.861056  \n",
      "94    55805.51       8.00     554.92  218.467621  \n",
      "99      260.36     983.65    4735.78  219.535990  \n",
      "105     101.64    8471.88    6886.04  220.265178  \n",
      "135      50.94    4120.32    2737.17  222.439015  \n",
      "146   74910.32     209.91     303.32  223.561947  \n",
      "151     174.78      74.46    1208.86  223.960414  \n",
      "157     280.89   20426.61    4671.78  224.235029  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset = [\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "print('----------------------------')\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset = [\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "print('----------------------------')\n",
    "# Subset the rows that are holiday weeks and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset = \"date\")\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting categorical variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Counting categorical variables\n",
    "\n",
    "Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you'll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "\n",
    "The store_types and store_depts DataFrames you created in the last exercise are available and pandas is imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Count the number of stores of each store type in store_types.\n",
    "    Count the proportion of stores of each store type in store_types.\n",
    "    Count the number of different departments in store_depts, sorting the counts in descending order.\n",
    "    Count the proportion of different departments in store_depts, sorting the proportions in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    39\n",
      "B     6\n",
      "C     2\n",
      "Name: type, dtype: int64\n",
      "-----------------------\n",
      "A    0.829787\n",
      "B    0.127660\n",
      "C    0.042553\n",
      "Name: type, dtype: float64\n",
      "-----------------------\n",
      "1    35\n",
      "3     7\n",
      "2     6\n",
      "Name: department, dtype: int64\n",
      "-----------------------\n",
      "1    0.729167\n",
      "3    0.145833\n",
      "2    0.125000\n",
      "Name: department, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stores of each type # This willl help identify how many groups(Type) like A,B,C avalibale in theType column\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "print('-----------------------')\n",
    "# Get the proportion of stores of each type # This will give how munch each group(A,B,C) distributed\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "print('-----------------------')\n",
    "# Count the number of each department number and sort # This will give sorted group of (A,B,C)\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\n",
    "print(dept_counts_sorted)\n",
    "print('-----------------------')\n",
    "# Get the proportion of departments of each number and sort # This will give sorted & distribution\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What percent of sales occurred at each store type? (Manual -Long steps)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What percent of sales occurred at each store type?\n",
    "\n",
    "While .groupby() is useful, you can calculate grouped summary statistics without it.\n",
    "\n",
    "Walmart distinguishes three types of stores: \"supercenters\", \"discount stores\", and \"neighborhood markets\", encoded in this dataset as type \"A\", \"B\", and \"C\". In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each.\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Calculate the total weekly sales over the whole dataset.\n",
    "    Subset for type \"A\" stores, and calculate their total weekly sales.\n",
    "    Do the same for type \"B\" and type \"C\" stores.\n",
    "    Combine the A/B/C results into a list, and divide by overall sales to get the proportion of sales by type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "44103257.1\n",
      "-----------------------\n",
      "6134490.0\n",
      "-----------------------\n",
      "1164486.2\n",
      "-----------------------\n",
      "[0.85800274 0.11934287 0.02265439]\n"
     ]
    }
   ],
   "source": [
    "# In this example we have three differnt types A,B,C we need to calculate the A Type weekly sales,B Type Sales\n",
    "# C Type Sale and then we meed to calculate the how much % from Total sales. We will use below method.\n",
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "print('-----------------------')\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "print(sales_A)\n",
    "print('-----------------------')\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "print(sales_B)\n",
    "\n",
    "print('-----------------------')\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "print(sales_C)\n",
    "print('-----------------------')\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations with .groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    44103257.1\n",
      "B     6134490.0\n",
      "C     1164486.2\n",
      "Name: weekly_sales, dtype: float64\n",
      "type\n",
      "A    0.858003\n",
      "B    0.119343\n",
      "C    0.022654\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Instead of above manual stpes we will use group my method and we will caluclate effetively.\n",
    "# Example\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "print(sales_by_type)\n",
    "print(sales_by_type[[\"A\", \"B\",\"C\"]] / sales_by_type.sum())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Calculations with .groupby()\n",
    "\n",
    "The `.groupby()` method makes life much easier. In this exercise, you'll perform the same calculations as last time, except you'll use the `.groupby()` method. You'll also perform calculations on data grouped by two variables to see if sales differs by store type depending on if it's a holiday week or not.\n",
    "\n",
    "`sales` is available and `pandas` is loaded as `pd`.\n",
    "\n",
    "**Instructions 1/2**\n",
    "\n",
    "* Group sales by `\"type\"`, take the sum of `\"weekly_sales\"`, and store as `sales_by_type`.\n",
    "* Calculate the proportion of sales at each store type by dividing by the sum of `sales_by_type`. Assign to `sales_propn_by_type`.\n",
    "\n",
    "**Instructions 2/2**\n",
    "\n",
    "* Group sales by `\"type\"` and `\"is_holiday\"`, take the sum of `weekly_sales`, and store as `sales_by_holiday_type`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    44103257.1\n",
      "B     6134490.0\n",
      "C     1164486.2\n",
      "Name: weekly_sales, dtype: float64\n",
      "type\n",
      "A    0.858003\n",
      "B    0.119343\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "print(sales_by_type)\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type[[\"A\", \"B\"]] / sales_by_type.sum()\n",
    "print(sales_propn_by_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  store  department  weekly_sales  temperature_c  \\\n",
      "type is_holiday                                                    \n",
      "A    False       128232        9285    41510530.6      392919.63   \n",
      "     True          9828         713     2592726.5       24288.86   \n",
      "B    False        38638         927     5763338.9       54471.07   \n",
      "     True          3042          73      371151.1        3472.75   \n",
      "C    False         8045         179     1101726.9       10391.52   \n",
      "     True           585          13       62759.3         583.43   \n",
      "\n",
      "                 fuel_price_usd_per_l  unemployment    MarkDown1   MarkDown2  \\\n",
      "type is_holiday                                                                \n",
      "A    False                  22185.676     47468.002  21768906.31  6031287.69   \n",
      "     True                    1638.405      3934.720   3331188.80  2804084.32   \n",
      "B    False                   3162.319      6138.572   2182690.20   549094.53   \n",
      "     True                     238.761       521.441    306658.95   240097.77   \n",
      "C    False                    625.749      1347.769    667479.37   184974.32   \n",
      "     True                      44.163       112.470     97599.41    75641.20   \n",
      "\n",
      "                  MarkDown3   MarkDown4    MarkDown5           CPI  \n",
      "type is_holiday                                                     \n",
      "A    False        769175.94  8540833.91  13071212.89  1.039535e+06  \n",
      "     True        4954645.06  1606419.85    811633.38  8.639669e+04  \n",
      "B    False         53824.29   816292.65   2402150.64  1.418421e+05  \n",
      "     True         468395.31   165797.58    126336.25  1.213733e+04  \n",
      "C    False         13043.51   244148.68    310034.24  2.921674e+04  \n",
      "     True         100157.84    33237.24     14109.11  2.436748e+03  \n"
     ]
    }
   ],
   "source": [
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\",\"is_holiday\"]).sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type  is_holiday\n",
      "A     False         41510530.6\n",
      "      True           2592726.5\n",
      "B     False          5763338.9\n",
      "      True            371151.1\n",
      "C     False          1101726.9\n",
      "      True             62759.3\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\",\"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple grouped summaries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Multiple grouped summaries\n",
    "\n",
    "Earlier in this chapter you saw that the `.agg()` method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as `np`, has many different summary statistics functions, including:\n",
    "\n",
    "* `np.min()`\n",
    "* `np.max()`\n",
    "* `np.mean()`\n",
    "* `np.median()`\n",
    "\n",
    "`sales` is available and `pandas` is imported as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Import NumPy with the alias `np`.\n",
    "* Get the min, max, mean, and median of `weekly_sales` for each store type using `.groupby()` and `.agg().` Store this as `sales_stats`. Make sure to use `numpy` functions!\n",
    "* Get the min, max, mean, and median of `unemployment` and `fuel_price_usd_per_l` for each store type. Store this as `unemp_fuel_stats`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        amin     amax         mean  median\n",
      "type                                      \n",
      "A     -440.1  10570.3  6302.265947  6443.1\n",
      "B     1275.3   9920.2  6134.490000  6273.1\n",
      "C     2728.7   8927.6  6065.032292  6104.2\n",
      "     unemployment                          fuel_price_usd_per_l         \\\n",
      "             amin    amax      mean median                 amin   amax   \n",
      "type                                                                     \n",
      "A           3.684  14.313  7.903248  7.852                2.472  4.468   \n",
      "B           3.684  10.641  7.145937  6.989                2.514  4.468   \n",
      "C           8.335   8.992  8.640467  8.625                2.699  4.066   \n",
      "\n",
      "                        \n",
      "          mean  median  \n",
      "type                    \n",
      "A     3.404413  3.5090  \n",
      "B     3.401080  3.5090  \n",
      "C     3.489125  3.6255  \n"
     ]
    }
   ],
   "source": [
    "# Import NumPy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[\"unemployment\", \"fuel_price_usd_per_l\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting on one variable  ( Alternate to group by)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weekly_sales## Pivoting on one variable\n",
    "\n",
    "Pivot tables are the standard way of aggregating data in spreadsheets. In pandas, pivot tables are essentially just another way of performing grouped calculations. That is, the `.pivot_table()` method is just an alternative to `.groupby()`.\n",
    "\n",
    "In this exercise, you'll perform calculations using `.pivot_table()` to replicate the calculations you performed in the last lesson using `.groupby()`.\n",
    "\n",
    "`sales` is available and `pandas` is imported as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions 1/3**\n",
    "\n",
    "* Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type.\n",
    "\n",
    "**Instructions 2/3**\n",
    "\n",
    "* Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type.\n",
    "\n",
    "**Instructions 3/3**\n",
    "\n",
    "* Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weekly_sales\n",
      "type              \n",
      "A      6302.265947\n",
      "B      6134.490000\n",
      "C      6065.032292\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\")\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mean\n",
      "     weekly_sales\n",
      "type             \n",
      "A     6302.265947\n",
      "B     6134.490000\n",
      "C     6065.032292\n",
      "             mean       median\n",
      "     weekly_sales weekly_sales\n",
      "type                          \n",
      "A     6302.265947       6443.1\n",
      "B     6134.490000       6273.1\n",
      "C     6065.032292       6104.2\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean])\n",
    "print(mean_sales_by_type)\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean, np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_holiday        False        True \n",
      "type                                \n",
      "A           6387.218126  5195.844689\n",
      "B           6217.194067  5084.261644\n",
      "C           6154.898883  4827.638462\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales by store type and holiday\n",
    "mean_sales_by_type_holiday = sales.pivot_table(values=\"weekly_sales\", index=\"type\", columns=\"is_holiday\")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in missing values and sum values with pivot tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Fill in missing values and sum values with pivot tables\n",
    "\n",
    "The `.pivot_table()` method has several useful arguments, including `fill_value` and `margins`.\n",
    "\n",
    "* `fill_value` replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course [(Dealing with Missing Data in Python)](https://www.datacamp.com/courses/dealing-with-missing-data-in-python), but the simplest thing to do is to substitute a dummy value.\n",
    "* `margins` is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.\n",
    "\n",
    "In this exercise, you'll practice using these arguments to up your pivot table skills, which will help you crunch numbers more efficiently!\n",
    "\n",
    "`sales` is available and `pandas` is imported as `pd`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions 1/2**\n",
    "\n",
    "Print the mean `weekly_sales` by `department` and `type`, filling in any missing values with `0`.\n",
    "\n",
    "**Instructions 2/2**\n",
    "\n",
    "Print the mean `weekly_sales` by `department` and `type`, filling in any missing values with `0` and summing all rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                  A        B            C\n",
      "department                                   \n",
      "1           6373.778451  6134.49  6065.032292\n",
      "2           6533.929900      NaN          NaN\n",
      "3           5713.182500      NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "# Print mean weekly_sales by department and type; \n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\"))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department            1          2          3\n",
      "type                                         \n",
      "A           6373.778451  6533.9299  5713.1825\n",
      "B           6134.490000        NaN        NaN\n",
      "C           6065.032292        NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "# Print mean weekly_sales by department and type; \n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"type\", columns=\"department\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                  A        B            C\n",
      "department                                   \n",
      "1           6373.778451  6134.49  6065.032292\n",
      "2           6533.929900     0.00     0.000000\n",
      "3           5713.182500     0.00     0.000000\n"
     ]
    }
   ],
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values with 0 ( From above)\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                 A        B          C         All\n",
      "department                                            \n",
      "1           31856144.7  6134490  1164486.2  39155120.9\n",
      "2            6533929.9        0        0.0   6533929.9\n",
      "3            5713182.5        0        0.0   5713182.5\n",
      "All         44103257.1  6134489  1164486.2  51402233.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols ( Statics of each col & row)\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0, aggfunc=sum, margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
